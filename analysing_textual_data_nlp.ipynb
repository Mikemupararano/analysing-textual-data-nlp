{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9005d1-a97e-448a-a0c7-d214dafea0fc",
   "metadata": {},
   "source": [
    "Text normalization-converting text to standard form for example to lowercase, handling incorrectly spelt words, and abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "810cf89c-c838-4bb6-bc3b-993bb61e6169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\1Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\1Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\1Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\1Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing important libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from spacy import displacy\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Downloading necessary NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7443598-ba02-4e39-bd79-f5e34497a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', '.']\n",
      "Filtered Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'Artificial', 'Intelligence', '.']\n",
      "Lemmatized Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'Artificial', 'Intelligence', '.']\n"
     ]
    }
   ],
   "source": [
    "#Confirmation text script\n",
    "#import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered Tokens:\", filtered_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b064555a-5f39-41c8-90a9-0223c87e17a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taj mahal is one of the beautiful monuments. it is one of the wonders of the world.\n",
      "it was built by shah jahan in 1631 in memory of his third beloved wife mumtaj mahal.\n"
     ]
    }
   ],
   "source": [
    "#Input text\n",
    "paragraph=\"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world.\n",
    "It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n",
    "#Convert paragraph to lowercase\n",
    "para_to_lowercase=paragraph.lower()\n",
    "print(para_to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcbc8052-14fc-43c5-a6fc-082277ff07cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Taj Mahal is one of the beautiful monuments.', 'It is one of the wonders of the world.', 'It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization-split the text into either tokens of words or token of sentences.\n",
    "#Tokenize paragraphs into sentences using sent_tokensize()\n",
    "tokenized_sentence=sent_tokenize(paragraph)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0f11e1a-83b9-403a-91d3-17d78bd18149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Taj Mahal is one of the beautiful monuments.', 'It is one of the wonders of the world.\\n', 'It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize paragraph into sentences using spacy:\n",
    "#Load englis language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#Build the nlp pipe using 'sentencizer'\n",
    "# sent_pipe=nlp.create_pipe('sentencizer')\n",
    "# #Append the sentencizer pipe to the nlp pipeline\n",
    "# nlp.add_pipe(sent_pipe)\n",
    "# # Create nlp Object to handle linguistic annotations in a documents.\n",
    "# my_doc = nlp(paragraph)\n",
    "# # Generate list of tokenized sentence\n",
    "# tokenized_sentences = []\n",
    "# for sentence in nlp_doc.sents:\n",
    "#     tokenized_sentences.append(sentence.text)\n",
    "# print(tokenized_sentences)\n",
    "# Ensure the 'sentencizer' pipe is added to the nlp pipeline\n",
    "if not nlp.has_pipe('sentencizer'):\n",
    "    nlp.add_pipe('sentencizer')\n",
    "\n",
    "# Example paragraph to tokenize\n",
    "paragraph = \"\"\"Taj Mahal is one of the beautiful monuments. It is one of the wonders of the world.\n",
    "It was built by Shah Jahan in 1631 in memory of his third beloved wife Mumtaj Mahal.\"\"\"\n",
    "\n",
    "# Create nlp object to handle linguistic annotations in a document\n",
    "my_doc = nlp(paragraph)\n",
    "\n",
    "# Generate list of tokenized sentences\n",
    "tokenized_sentences = [sentence.text for sentence in my_doc.sents]\n",
    "\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf2d6b-68bc-4046-8c84-385461329447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
